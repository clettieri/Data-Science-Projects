{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sberbank: Russian Housing Market - Part 2\n",
    "\n",
    "url = https://www.kaggle.com/c/sberbank-russian-housing-market\n",
    "\n",
    "### Improving the Model\n",
    "\n",
    "In this notebook I am going to build on some of the work I did previously in part 1.  I will load in my data and functions from part 1 and spend most of this notebook feature engineering to improve my model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is one idea I was trying - in the link provided, there is the case made for only using certain macro columns b/c the rest do not seem to provide any new information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# From here: https://www.kaggle.com/robertoruiz/sberbank-russian-housing-market/dealing-with-multicollinearity/notebook\n",
    "macro_cols = [\"balance_trade\", \"balance_trade_growth\", \"eurrub\", \"average_provision_of_build_contract\",\n",
    "\"micex_rgbi_tr\", \"micex_cbi_tr\", \"deposits_rate\", \"mortgage_value\", \"mortgage_rate\",\n",
    "\"income_per_cap\", \"rent_price_4+room_bus\", \"museum_visitis_per_100_cap\", \"apartment_build\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load Data\n",
    "macro_raw = pd.read_csv('data/macro.csv', parse_dates=['timestamp']) #Load all macro data\n",
    "#macro_raw = pd.read_csv('data/macro.csv', parse_dates=['timestamp'], usecols=['timestamp']+macro_cols) #Load only macro_cols\n",
    "train_raw = pd.read_csv('data/train.csv', parse_dates=['timestamp'])\n",
    "test_raw = pd.read_csv('data/test.csv', parse_dates=['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Join macro-economic data\n",
    "train_full = pd.merge(train_raw, macro_raw, how='left', on='timestamp')\n",
    "test_full = pd.merge(test_raw, macro_raw, how='left', on='timestamp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "##### First encode categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def encode_object_features(train, test):\n",
    "    '''(DataFrame, DataFrame) -> DataFrame, DataFrame\n",
    "    \n",
    "    Will encode each non-numerical column.\n",
    "    '''\n",
    "    train = pd.DataFrame(train)\n",
    "    test = pd.DataFrame(test)\n",
    "    cols_to_encode = train.select_dtypes(include=['object'], exclude=['int64', 'float64']).columns\n",
    "    for col in cols_to_encode:\n",
    "        le = LabelEncoder()\n",
    "        #Fit on both sets of data\n",
    "        le.fit(train[col].append(test[col]))\n",
    "        #Transform each\n",
    "        train[col] = le.transform(train[col])\n",
    "        test[col] = le.transform(test[col])\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df, test_df = encode_object_features(train_full, test_full);\n",
    "#train_df, test_df = encode_object_features(train_raw, test_raw)  #No Macro Data - tried once not using any macro data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_date_features(df):\n",
    "    '''(DataFrame) -> DataFrame\n",
    "    \n",
    "    Will add some specific columns based on the date\n",
    "    of the sale.\n",
    "    '''\n",
    "    #Convert to datetime to make extraction easier\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    #Extract features\n",
    "    df['month'] = df['timestamp'].dt.month\n",
    "    df['day'] = df['timestamp'].dt.day\n",
    "    df['year'] = df['timestamp'].dt.year\n",
    "    \n",
    "    #These features inspired by Bruno's Notebook at https://www.kaggle.com/bguberfain/naive-xgb-lb-0-317\n",
    "    #Month-Year\n",
    "    month_year = df['timestamp'].dt.month + df['timestamp'].dt.year * 100\n",
    "    month_year_map = month_year.value_counts().to_dict()\n",
    "    df['month_year'] = month_year.map(month_year_map)\n",
    "    #Week-Year\n",
    "    week_year = df['timestamp'].dt.weekofyear + df['timestamp'].dt.year * 100\n",
    "    week_year_map = week_year.value_counts().to_dict()\n",
    "    df['week_year'] = week_year.map(week_year_map)\n",
    "    df.drop('timestamp', axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_state_features(df):\n",
    "    '''(DataFrame) -> DataFrame\n",
    "    \n",
    "    Add's features, meant to be used for both train and test df's.\n",
    "    Does operations to the state grouping\n",
    "    '''\n",
    "    #Get median of full sq by state\n",
    "    df['state_median_full_sq'] = df['full_sq'].groupby(df['state']).transform('median')\n",
    "    #Build features from full sq median by state\n",
    "    df['full_sq_state_median_diff'] = df['full_sq'] - df['state_median_full_sq']\n",
    "    df['life_sq_state_median_full_diff'] = df['life_sq'] - df['state_median_full_sq']\n",
    "    #Drop helper columns\n",
    "    df.drop('state_median_full_sq', axis=1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_features(df):\n",
    "    '''(DataFrame) -> DataFrame\n",
    "    \n",
    "    Add's features, meant to be used for both train and test df's.\n",
    "    '''\n",
    "    #Floor\n",
    "    df['floor_ratio'] = df['floor'] / df['max_floor'].astype(float)\n",
    "    df['floor_from_top'] = df['max_floor'] - df['floor']\n",
    "    #Sq areas\n",
    "    df['kitch_sq_ratio'] = df['kitch_sq'] / df['full_sq'].astype(float)\n",
    "    df['life_sq_ratio'] = df['life_sq'] / df['full_sq'].astype(float)\n",
    "    df['full_sq_per_room'] = df['full_sq'] / df['num_room'].astype(float)\n",
    "    df['life_sq_per_room'] = df['life_sq'] / df['num_room'].astype(float)\n",
    "    df['full_living_sq_diff'] = df['full_sq'] - df['life_sq']\n",
    "    #df['full_sq_per_floor'] = df['full_sq'] / df['max_floor'].astype(float) #No value added\n",
    "    df = add_date_features(df)\n",
    "    df = add_state_features(df)\n",
    "    df['build_year_vs_year_diff'] = df['build_year'] - df['year']  #no change\n",
    "    \n",
    "    #Drop Id -> Made it worse\n",
    "    #df.drop('id', axis=1, inplace=True)\n",
    "    \n",
    "    #School Variables -> Made it worse\n",
    "    #df['preschool_quota_ratio'] = df[\"children_preschool\"] / df[\"preschool_quota\"].astype(\"float\")\n",
    "    #df['school_quota_ratio'] = df[\"children_school\"] / df[\"school_quota\"].astype(\"float\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = add_features(train_df)\n",
    "test_df = add_features(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30471, 405)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validate\n",
    "\n",
    "Here I use cross-validation to test my new features.  After, I also train a model to take a look at the feature_importances_ as determined by the XGB algorithm.  These importances can give you ideas of which features to focus on for further feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.20403424895\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "\n",
    "#Get Data\n",
    "#Y_train = train_df['price_doc'].values\n",
    "Y_train = np.log1p(train_df['price_doc'].values)\n",
    "X_train = train_df.ix[:, train_df.columns != 'price_doc'].values\n",
    "X_test = test_df.values\n",
    "\n",
    "#Initialize Model\n",
    "xgb = XGBRegressor()\n",
    "#Create cross-validation\n",
    "cv = TimeSeriesSplit(n_splits=5)\n",
    "#Train & Test Model\n",
    "cross_val_results = cross_val_score(xgb, X_train, Y_train, cv=cv, scoring='neg_mean_squared_error')\n",
    "print cross_val_results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "      <th>feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.144118</td>\n",
       "      <td>full_sq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.023529</td>\n",
       "      <td>id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.023529</td>\n",
       "      <td>state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.023529</td>\n",
       "      <td>floor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.019118</td>\n",
       "      <td>railroad_km</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.019118</td>\n",
       "      <td>green_zone_km</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.017647</td>\n",
       "      <td>ttk_km</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.016176</td>\n",
       "      <td>product_type</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>0.016176</td>\n",
       "      <td>build_year_vs_year_diff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.014706</td>\n",
       "      <td>nuclear_reactor_km</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.013235</td>\n",
       "      <td>build_year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.013235</td>\n",
       "      <td>mkad_km</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.013235</td>\n",
       "      <td>swim_pool_km</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>0.011765</td>\n",
       "      <td>prom_part_1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>0.011765</td>\n",
       "      <td>cafe_count_5000_price_2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>0.011765</td>\n",
       "      <td>cafe_count_5000_price_high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>0.010294</td>\n",
       "      <td>cafe_count_3000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>0.010294</td>\n",
       "      <td>trc_count_1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.008824</td>\n",
       "      <td>indust_part</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.008824</td>\n",
       "      <td>industrial_km</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.008824</td>\n",
       "      <td>metro_km_avto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.008824</td>\n",
       "      <td>metro_min_avto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0.008824</td>\n",
       "      <td>public_healthcare_km</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>0.008824</td>\n",
       "      <td>micex_cbi_tr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0.008824</td>\n",
       "      <td>additional_education_km</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.008824</td>\n",
       "      <td>max_floor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.008824</td>\n",
       "      <td>railroad_station_avto_km</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>0.008824</td>\n",
       "      <td>full_sq_state_median_diff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0.007353</td>\n",
       "      <td>prom_part_500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.007353</td>\n",
       "      <td>ID_railroad_station_avto</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     importance                     feature\n",
       "1      0.144118                     full_sq\n",
       "0      0.023529                          id\n",
       "9      0.023529                       state\n",
       "3      0.023529                       floor\n",
       "116    0.019118                 railroad_km\n",
       "91     0.019118               green_zone_km\n",
       "107    0.017647                      ttk_km\n",
       "10     0.016176                product_type\n",
       "403    0.016176     build_year_vs_year_diff\n",
       "123    0.014706          nuclear_reactor_km\n",
       "6      0.013235                  build_year\n",
       "106    0.013235                     mkad_km\n",
       "131    0.013235                swim_pool_km\n",
       "176    0.011765              prom_part_1000\n",
       "281    0.011765  cafe_count_5000_price_2500\n",
       "283    0.011765  cafe_count_5000_price_high\n",
       "250    0.010294             cafe_count_3000\n",
       "202    0.010294              trc_count_1500\n",
       "15     0.008824                 indust_part\n",
       "92     0.008824               industrial_km\n",
       "85     0.008824               metro_km_avto\n",
       "84     0.008824              metro_min_avto\n",
       "137    0.008824        public_healthcare_km\n",
       "309    0.008824                micex_cbi_tr\n",
       "142    0.008824     additional_education_km\n",
       "4      0.008824                   max_floor\n",
       "99     0.008824    railroad_station_avto_km\n",
       "401    0.008824   full_sq_state_median_diff\n",
       "153    0.007353               prom_part_500\n",
       "101    0.007353    ID_railroad_station_avto"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = xgb.fit(X_train, Y_train)\n",
    "importances = zip(model.feature_importances_, train_df.ix[:, train_df.columns != 'price_doc'].columns)\n",
    "importances = pd.DataFrame(importances, columns=['importance', 'feature'])\n",
    "importances.sort_values('importance', ascending=False).head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CV Results\n",
    "\n",
    "Below are notes to myself on some of the CV scores from trying different feature combinations.\n",
    "\n",
    "Top 20 +18 ratios & Default = -8.652 x10^12  \n",
    "Top 20 only & Default = -8.652 x10^12  \n",
    "27 Features (all ratios) = -8.59286663107e+12    \n",
    "22 Features (removed some ratios) = -8.59753424352e+12  \n",
    "28 (full sq per floor) = -8.64215439693e+12  \n",
    "All Base = -7.98651473264e+12  \n",
    "Base + date, ratios = -7.85642839197e+12  \n",
    "Base + best + ratio full to state median = -7.87055210987e+12  \n",
    "Base + best + difference full to state median = -7.85431265712e+12    \n",
    "Base + best + differences to state median = -7.83445282066e+12  \n",
    "Base + all so far + NO MACRO = -7.83445282066e+12  \n",
    "@Base + All (broken date here and up) = -7.809975808e+12 == -0.20160089611 log  \n",
    "Base + All -Id = -7.94649835405e+12 \n",
    "Base + All + fixed date = -0.20403424895  == -7.84  \n",
    "*Base + All + fixed date -Id = -0.201769935446  \n",
    "Base + All fixed-id w/Few Macro = -0.203388567584  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model & Submit\n",
    "\n",
    "Here I train my final model using the features I determined to use above.  Then I create a submission csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "#Get Data\n",
    "Y_train = train_df['price_doc'].values\n",
    "X_train = train_df.ix[:, train_df.columns != 'price_doc'].values\n",
    "X_test = test_df.values\n",
    "#Init Model\n",
    "xgb = XGBRegressor(learning_rate=0.05, max_depth=6, subsample=0.8, colsample_bytree=0.7)\n",
    "#Train Model\n",
    "model = xgb.fit(X_train, Y_train)\n",
    "#Make Predictions\n",
    "predictions = xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Make Submission File\n",
    "submission_df = pd.DataFrame({'id':test_full['id'], 'price_doc':predictions})\n",
    "submission_df.to_csv('xgb-added_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "Base + Date & Default XGBRegressor = 0.33413  \n",
    "Base Top 20 & Default XGBRegressor = 0.34833  \n",
    "Base Top 20 + 18 ratios & Default  = 0.34833   \n",
    "Base + All Features & Default     = 0.33386  \n",
    "Base + All Features & Tuned 317    = 0.32671  \n",
    "Base + All + No Macro & Tuned 317  = 0.32832  \n",
    "Base + All + fixed date -Id (-0.20177 cv) = 0.3260  \n",
    "*Base + All + fixed date +Id (-0.20160 cv) = 0.32552"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Next Steps\n",
    "\n",
    "Currently my best submission as me at 53% on the leaderboard.  \n",
    "\n",
    "Some next steps I want to take are:  \n",
    "-Engineer more features - play with groupby (sub_area, more state features, etc)  \n",
    "-Remove some features - try PCA? Learn more about when to remove features that aren't adding new info, how to tell  \n",
    "-NaN's - there is a lot of missing data and wrong data - should I remove some/all? Correct some (ie. incorrect years)?  \n",
    "-ID field seems to improve results - why is this? should I remove it anyway?\n",
    "-Optimize XGB using GridSearch and Build Ensemble learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
